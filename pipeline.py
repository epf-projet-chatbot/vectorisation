"""
Pipeline compl√®te de traitement des documents pour le chatbot juridique

MODES D'UTILISATION :

1. MODE TEST (--test) :
   - Utilise un petit dataset de 7 fichiers (3 MD, 3 PDF, 1 JSON)
   - Donn√©es dans ./data_test/
   - Id√©al pour d√©veloppement et validation rapide
   - Commande : python pipeline.py --test

2. MODE PRODUCTION (par d√©faut) :
   - Traite tous les documents du projet
   - Donn√©es dans ./data/
   - Pour d√©ploiement en production
   - Commande : python pipeline.py

√âTAPES DU PIPELINE :
1. Chargement des documents (Markdown, PDF, JSON)
2. D√©coupage en chunks avec chevauchement
3. G√©n√©ration des embeddings (multilingual-e5-small)
4. Insertion en MongoDB par lots
"""

import os
import argparse
import sys
from typing import Optional

# V√©rifier l'argument --test au d√©marrage pour configurer l'environnement
if "--test" in sys.argv:
    os.environ["TEST_MODE"] = "true"
    print("üß™ Mode TEST activ√© via argument --test")
elif "--prod" in sys.argv or "--production" in sys.argv:
    os.environ["TEST_MODE"] = "false"
    print("üè≠ Mode PRODUCTION activ√© via argument --prod/--production")

from loader import load_all_documents
from chunker import process_documents_chunks
from embedder import process_chunks_embeddings
from mongo import insert_chunks_batch, clear_collection, get_collection_stats
from config import config
from preprocessor import preprocess_text

def run_pipeline(chunk_size: int = 1000, overlap: int = 200, clear_db: bool = False, test_mode: bool = False):
    """
    Ex√©cute la pipeline compl√®te de traitement des documents
    
    Args:
        chunk_size: Taille maximale des chunks en caract√®res
        overlap: Chevauchement entre les chunks
        clear_db: Si True, vide la base de donn√©es avant l'insertion
        test_mode: Si True, utilise les donn√©es de test (./data_test/)
    """
    # Mise √† jour de la configuration globale
    config.test_mode = test_mode
    config.chunk_size = chunk_size
    config.chunk_overlap = overlap
    
    mode_text = "MODE TEST" if test_mode else "MODE PRODUCTION"
    print("=" * 60)
    print(f"{mode_text} - PIPELINE DE VECTORISATION")
    print("=" * 60)
    
    try:
        # Optionnel : vider la base de donn√©es
        if clear_db:
            print("\nNettoyage de la base de donn√©es...")
            clear_collection()
        
        # √âtape 1: Chargement des documents
        print("\nETAPE 1: Chargement des documents")
        print("-" * 40)
        documents = load_all_documents()
        
        if not documents:
            print("Aucun document trouv√©. Arr√™t de la pipeline.")
            return
        
        print(f"{len(documents)} documents charg√©s avec succ√®s")
        
        # √âtape 2: D√©coupage en chunks
        print(f"\nETAPE 2: D√©coupage en chunks")
        print("-" * 40)
        print(f"Param√®tres: chunk_size={chunk_size}, overlap={overlap}")
        chunks = process_documents_chunks(documents, chunk_size, overlap)
        
        if not chunks:
            print("Aucun chunk cr√©√©. Arr√™t de la pipeline.")
            return
        
        print(f"{len(chunks)} chunks cr√©√©s avec succ√®s")
        
        # √âtape 2.2: Pr√©-traitement des chunks
        print(f"\nETAPE 2.2: Pr√©-traitement des chunks")
        print("-" * 40)
        for chunk in chunks:
            # Stocker le contenu original
            chunk['original_content'] = chunk['content']
            # Cr√©er le contenu pr√©trait√© pour les embeddings
            chunk['preprocessed_content'] = preprocess_text(chunk['content'])
        print("Pr√©-traitement des chunks termin√©")
        
        # √âtape 3: G√©n√©ration des embeddings
        print(f"\nETAPE 3: G√©n√©ration des embeddings")
        print("-" * 40)
        chunks_with_embeddings = process_chunks_embeddings(chunks)
        
        # √âtape 4: Insertion dans MongoDB
        print(f"\nETAPE 4: Insertion dans MongoDB")
        print("-" * 40)
        insert_chunks_batch(chunks_with_embeddings, batch_size=config.batch_size)
        
        # Statistiques finales
        print(f"\nSTATISTIQUES FINALES")
        print("-" * 40)
        print(f"Documents trait√©s: {len(documents)}")
        print(f"Chunks cr√©√©s: {len(chunks)}")
        print(f"Embeddings g√©n√©r√©s: {len(chunks_with_embeddings)}")
        
        stats = get_collection_stats()
        print(f"Documents en base: {stats['total_documents']}")
        
        print("\n" + "=" * 60)
        print("PIPELINE TERMIN√âE AVEC SUCC√àS")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nERREUR DANS LA PIPELINE: {str(e)}")
        raise

def main():
    """Point d'entr√©e principal avec arguments en ligne de commande"""
    parser = argparse.ArgumentParser(description="Pipeline de vectorisation de documents")
    parser.add_argument("--chunk-size", type=int, default=1000, 
                       help="Taille maximale des chunks en caract√®res (d√©faut: 1000)")
    parser.add_argument("--overlap", type=int, default=200,
                       help="Chevauchement entre chunks en caract√®res (d√©faut: 200)")
    parser.add_argument("--clear-db", action="store_true",
                       help="Vider la base de donn√©es avant l'insertion")
    parser.add_argument("--stats-only", action="store_true",
                       help="Afficher uniquement les statistiques de la DB")
    parser.add_argument("--test", action="store_true",
                       help="Utiliser les donn√©es de test (mode test)")
    parser.add_argument("--prod", "--production", action="store_true",
                       help="Forcer le mode production (explicite)")
    
    args = parser.parse_args()
    
    # D√©terminer le mode √† utiliser
    test_mode = args.test
    if args.prod:
        test_mode = False
    
    if args.stats_only:
        from mongo import get_collection_stats
        print("üìä Statistiques de la base de donn√©es:")
        stats = get_collection_stats()
        print(f"Total documents: {stats['total_documents']}")
        print(f"Fichiers uniques: {stats['unique_files']}")
        print(f"Mode: {'TEST' if stats.get('test_mode', False) else 'PRODUCTION'}")
        for ext, count in stats.get('file_types', {}).items():
            print(f"  .{ext}: {count} fichier(s)")
        return
    
    run_pipeline(
        chunk_size=args.chunk_size,
        overlap=args.overlap,
        clear_db=args.clear_db,
        test_mode=test_mode
    )

if __name__ == "__main__":
    main()
