"""
ğŸ§  ARCHITECTURE COMPLÃˆTE DU SYSTÃˆME RAG
======================================

ğŸ“‹ MODÃˆLES ET LEUR LOCALISATION :
===============================

1. ğŸ”¤ MODÃˆLE D'EMBEDDING (Vectorisation)
   ğŸ“ DÃ©fini dans: .env â†’ EMBEDDING_MODEL=intfloat/multilingual-e5-small
   ğŸ“ UtilisÃ© dans: embedder.py (ligne 8)
   ğŸ­ Fournisseur: Hugging Face (sentence-transformers)
   ğŸ“ Dimension: 384 vecteurs
   ğŸ¯ RÃ´le: Convertit le texte en vecteurs numÃ©riques

2. ğŸ¤– MODÃˆLE LLM (GÃ©nÃ©ration de rÃ©ponses)
   ğŸ“ DÃ©fini dans: rag_performance_test.py (ligne 61)
   ğŸ“ ModÃ¨le: "llama-3.3-70b-versatile"
   ğŸ­ Fournisseur: Groq (API)
   ğŸ”‘ API Key: .env â†’ API_KEY=gsk_dzYefK...
   ğŸ¯ RÃ´le: GÃ©nÃ¨re la rÃ©ponse finale basÃ©e sur le contexte

ğŸ”„ PROCESSUS COMPLET DE VECTORISATION :
=====================================

PHASE 1: VECTORISATION INITIALE (pipeline.py)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CHARGEMENT (loader.py)                      â”‚
â”‚    ğŸ“„ PDF â†’ texte (PyMuPDF)                   â”‚
â”‚    ğŸ“ Markdown â†’ texte                         â”‚
â”‚    ğŸ“Š JSON â†’ texte                             â”‚
â”‚                                                â”‚
â”‚ 2. CHUNKING (chunker.py)                      â”‚
â”‚    ğŸ“ Taille: 1000 caractÃ¨res                 â”‚
â”‚    ğŸ”— Overlap: 200 caractÃ¨res                 â”‚
â”‚                                                â”‚
â”‚ 3. PREPROCESSING (preprocessor.py)             â”‚
â”‚    ğŸ§¹ Nettoyage du texte                      â”‚
â”‚    ğŸ”¤ Normalisation                           â”‚
â”‚                                                â”‚
â”‚ 4. EMBEDDING (embedder.py)                    â”‚
â”‚    ğŸ§  ModÃ¨le: intfloat/multilingual-e5-small â”‚
â”‚    ğŸ“ 384 dimensions par chunk                 â”‚
â”‚                                                â”‚
â”‚ 5. STOCKAGE (mongo.py)                        â”‚
â”‚    ğŸ’¾ MongoDB: chatbot_test_db.data          â”‚
â”‚    ğŸ“‹ Structure: {content, embedding, source} â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 2: REQUÃŠTE UTILISATEUR (rag.py + rag_performance_test.py)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. QUESTION UTILISATEUR                        â”‚
â”‚    â“ "Quel est le nombre de litiges ?"        â”‚
â”‚                                                â”‚
â”‚ 2. VECTORISATION QUESTION (rag.py:make_vector) â”‚
â”‚    ğŸ§  MÃªme modÃ¨le: multilingual-e5-small      â”‚
â”‚    ğŸ“ 384 dimensions                           â”‚
â”‚                                                â”‚
â”‚ 3. RECHERCHE SIMILARITÃ‰ (rag.py:k_context)    â”‚
â”‚    ğŸ” Algorithme: NearestNeighbors (sklearn)  â”‚
â”‚    ğŸ“Š RÃ©cupÃ¨re k=5 chunks les plus proches    â”‚
â”‚                                                â”‚
â”‚ 4. GÃ‰NÃ‰RATION RÃ‰PONSE (rag_performance_test)   â”‚
â”‚    ğŸ¤– LLM: llama-3.3-70b-versatile (Groq)    â”‚
â”‚    ğŸ”‘ API Groq avec votre clÃ©                 â”‚
â”‚    ğŸ“ Prompt + contexte â†’ rÃ©ponse             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”§ CONFIGURATION DÃ‰TAILLÃ‰E :
===========================

ğŸ“ FICHIER .env (Configuration globale)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EMBEDDING_MODEL=intfloat/multilingual-e5-small â”‚
â”‚ API_KEY=gsk_dzYefK5aUiuJmeeofFvsWG... â”‚
â”‚ CHUNK_SIZE=1000                     â”‚
â”‚ CHUNK_OVERLAP=200                   â”‚
â”‚ TEST_MODE=true                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“„ EMBEDDER.PY (Vectorisation)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ def get_embedding(text: str):       â”‚
â”‚   return model.encode(text).tolist()â”‚
â”‚                                     â”‚
â”‚ ModÃ¨le chargÃ© ligne 8:              â”‚
â”‚ model = SentenceTransformer(config.embedding_model) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¤– RAG_PERFORMANCE_TEST.PY (LLM)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ client = Groq(api_key=os.environ.get("API_KEY")) â”‚
â”‚                                     â”‚
â”‚ chat_completion = client.chat.completions.create( â”‚
â”‚   model="llama-3.3-70b-versatile"  â”‚
â”‚   messages=[...]                   â”‚
â”‚ )                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ” PROBLÃˆME IDENTIFIÃ‰ DANS VOTRE RAG :
====================================

âŒ Le chunking fragmente les informations importantes
âŒ "55 litiges" est probablement coupÃ© entre plusieurs chunks
âŒ La recherche de similaritÃ© ne trouve pas le bon chunk

ğŸ’¡ SOLUTIONS :
- RÃ©duire CHUNK_SIZE Ã  500-700
- Augmenter CHUNK_OVERLAP Ã  300
- RÃ©cupÃ©rer plus de chunks (k=10-15 au lieu de 5)
- AmÃ©liorer le prompt pour Ãªtre plus prÃ©cis

ğŸ“Š FLUX DE DONNÃ‰ES :
==================

Documents â†’ Chunks â†’ Embeddings â†’ MongoDB
    â†“
Question utilisateur â†’ Embedding question
    â†“
Recherche similaritÃ© dans MongoDB
    â†“
RÃ©cupÃ©ration des meilleurs chunks
    â†“
Envoi Ã  Groq LLM avec prompt
    â†“
RÃ©ponse gÃ©nÃ©rÃ©e
"""
